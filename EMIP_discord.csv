"","AuthorID","Author","Date","Content","Attachments","Reactions","channel","wordcount"
"1",304446699206868992,"zombieowl#4403","01-Jun-20 10:07 AM","‚ùì Question: To all the eye tracking experts in this workshop, can you tell us the ""why"" do you use eye tracking?  Next, could you briefly state what aspect of eye tracking is interesting and/or frustrating to you? @RB @Andrew Duchowski @abegel @OldCanada Please also give a short intro about what you do.",NA,"","general-discussion",54
"2",714635229771595776,"abegel#2066","01-Jun-20 11:45 AM","Hi! I use eye tracking to study programming, but I really like it more for acquiring incidental signals about a programmer's cognition as they work. Rather than use gaze points to power an interaction with the computer, I like to use that information to facilitate communication between software developers. You'd be surprised how much more effective collaborators can be when they know where the other one is looking. (",NA,"üëçüèΩ (3)","general-discussion",70
"3",714635229771595776,"abegel#2066","01-Jun-20 11:48 AM","The main frustration I have with eye tracking is the difference between the mainline story of what it can do vs. what it actually can do given limitations in the technology. While it can tell you an (x, y) location on the screen where someone is looking, it can't tell you what they're looking at, why they're looking at it, whether their gaze was meaningful, and where that location fits into the sequence of what they've looked at. Ultimately, I really want to know a programmer's cognition, which is impossible. There's a lot of promise in getting some hints about what programmers might be thinking, but it'll take a lot of creative experiments and triangulation with many kinds of sensors to nail this down with any confidence.",NA,"üíØ (3),üëç (3)","general-discussion",134
"4",521698252983238656,"OldCanada#6524","01-Jun-20 01:35 PM","Mike Dodd here (just realized that isn't clear from my discord name), I'm a cognitive psychologist who has long studied the factors that influence how attention is allocated throughout the visual field.  Because attention is dissociable from eye movements (e.g. these tend to be highly correlated but you can attend to something you aren't looking at), all classic attention paradigms require individuals to fixate on a central point and then respond via button press to events occurring on the screen.  Your speed of response is a proxy for attention.  There have been a number of excellent experiments in this regard that really inform our understanding of how attention moves, how it is impacted by various factors, etc.  Unfortunately, while these studies were incredibly necessary, they are also artificial, as there are few situations in the real world where you would fixate on a single point and shift attention around without moving your head and your eyes.  So after I was fully trained in those paradigms and background, I got more interested in studying attention in the real world (you know the whole crisis of whether what I'm doing is actually applicable in the real world) so I went about learning eyetracking so I could more realistically study the things I was studying artificially.",NA,"üëç (3)","general-discussion",218
"5",521698252983238656,"OldCanada#6524","01-Jun-20 01:48 PM","All of that background though also ties into my main frustration with eyetracking.  I was very well trained in low level attention such that I understand the disconnect between the eyes and attention, I know how to interpret eye movements and what you can meaningfully extract from them, and I know when to not overstate or overtly speculate on things.  But that is not the case for many people using eyetracking (and to be clear, that's not their fault either, you wouldn't expect someone in computer science to decide to do years of training in low level attention).  This creates a large problem because unlike ERP and fMRI (which can face similar issues but are more expensive so less frequently used by non experts), eyetracking is relatively cheap to do and accessible for a larger % of researchers.  It makes sense, therefore, that people would be excited to apply to their area, but often times the first and most influential papers in the field are not done by experts and quite frequently reach conclusions that just aren't supported by the data.  You can't infer someone's thought processes or complicated strategies based solely on where they look.  I really strive when collaborating with people in other areas to treat myself as the person who is most responsible for getting all the results/discussion/conclusions right to ensure that people aren't inappropriately stating what their results mean.  Once that happens with a seminal paper in an area, it makes it harder to step back and correct.  I think eyetracking might be one of the most important tools you can use to understand attention in the real world, but it needs to be combined with additional measures and additional expertise to make sure folks are not reading too much into what they are reporting, which happens a lot unfortunately",NA,"‚ù§Ô∏è (3),üëç (1)","general-discussion",311
"6",416250875393081344,"njssnjss#0837","01-Jun-20 02:03 PM","For me the analysis of eye-tracking data from experiments is quite difficult, especially when you start looking at relationships between sequences of eye-gaze, interactions, other sensed events, exploration of data, and for example recommendations from a recommender system.",NA,"","general-discussion",40
"7",521698252983238656,"OldCanada#6524","01-Jun-20 05:18 PM","Part of the issue here üëÜ is that there are not necessarily ""standard"" approaches for examining these things.  Even companies that produce eyetrackers have different labels/terms for concepts, different things build into their software, different output methods, and this all makes it difficult to figure out the ""right"" way to do things (similar issues again for EEG and fMRI in that there isn't a gold standard way to analyze).  So a lot of it becomes making decisions based on what makes sense, and knowing the other factors that could be confounding your results (e.g. luminance changes, violations of expectation, and strong contrast can all impact eye movements beyond just the general task being investigated",NA,"üëç (3)","general-discussion",117
"8",497819419599306752,"Andrew Duchowski#8309","01-Jun-20 07:48 PM","I'm just starting to use eye trackers to study programmers' attention so I don't yet have first-hand experience in this particular realm.  However, why I want to is so that I can try and glean attentional behavior from the practice of programmers, esp. when it comes to expert/novice comparisons and training.  The EMME paper hints at the latter, for example, and is indeed a compelling research direction.",NA,"üëçüèΩ (3),üëç (1)","general-discussion",71
"9",497819419599306752,"Andrew Duchowski#8309","01-Jun-20 07:52 PM","Apart from the ""why"", my approach would be to try and exploit metrics such as the K coefficient or transition matrix entropy to see if we can elicit patterns in expert/novice code ""scanning"" (code review, debugging, etc.).  I am also interested to see if cognitive load metrics would yield anything of interest.  Shameless plug: our CHI 2020 paper on the Low/High Index of Pupillary Activity (LHIPA) seemed to give us indication of cognitive load vs. baseline in three different tasks.  I don't know if it would yield differences in cognitive load between experts and novices.  If anyone gets something promising, please do let me know.",NA,"üëç (1)","general-discussion",108
"10",497819419599306752,"Andrew Duchowski#8309","01-Jun-20 08:00 PM","Eye tracking annoyances: calibration is the obvious, oft-given one, but apart from that it is access to the raw data that sometimes frustrates me.  Yes, getting fixations out of the box is nice, but sometimes the details of what filters are used are omitted (for proprietary reasons).  What else?  I'm sure there are other annoying aspects üôÇ  Well, one is inconsistent data labeling and recording.  This seems to happen more often than it should.  I am sometimes faced with data that is missing participant numbers, or they are inconsistently labelled, or the trial encoding is missing, or there are NaN data for whatever reason.  This sort of thing is frustrating, and worse yet, can lead to having to throw away data.  This has to do with experimental design and the act of running experiments.  Be like a carpenter: measure twice, cut once.  You might only get to see a participant once, so make sure their data counts! üôÇ",NA,"‚ù§Ô∏è (5)","general-discussion",160
"11",600770279026982912,"peitek#2292","02-Jun-20 03:08 AM","Thank you all for your detailed and thoughtful responses! 

My personal struggle with eye tracking is that there is a plethora of ways to analyze the data, but as far as I'm aware no standards on how to choose the right one. If I have a research question, I'm confident with the experiment design and data collection, but what analysis do I use to answer my research questions in a robust, reliable way? Fixation and saccades metrics? AOI transitions? Higher-level metrics such as pattern analysis? I wish there would be more of guidance/examples which measure makes sense for what kind of questions.",NA,"üíØ (2)","general-discussion",106
"12",716972004665851904,"RB#1006","03-Jun-20 03:17 AM","I am so amazed by the wave of papers doing fMRI on programmers and in combination with gaze-tracking. Who would have thought about this some 10 years ago. So glad this year's EMIP brought these networks together and gave a platform for the discussion.",NA,"üëçüèΩ (4),‚ù§Ô∏è (2)","general-discussion",46
"13",304446699206868992,"zombieowl#4403","03-Jun-20 09:04 AM","Ok, I will try to answer the question I posed as well. I use eye tracking to get some insight into the thought processes of developers.  Eye tracking is a close proxy to what they are thinking but not always. I am currently looking into expertise formulation and trying to determine how we can differentiate expertise based on eye movements.  My main annoyance with eye tracking is calibration and that the vendors need to do better in telling us how some measures are calculated especially if one does not necessarily want to use their tools or experiment suites.  Thi is the only way to compare across studies that use/don't use experimental suites. They have considerably improved but more needs to be done.",NA,"","general-discussion",124
"15",304446699206868992,"zombieowl#4403","03-Jun-20 09:06 AM","@everyone please feel free to also answer the question posed above.  For easy access to the questions, I have started to pin them.",NA,"","general-discussion",24
"16",707150407104724992,"Marjaana Puurtinen#3084","03-Jun-20 09:14 AM","Ok, @zombieowl , I will answer, too. üôÇ I have used eye tracking for music and text reading studies, and also piloted the method in food choice and face-to-face interaction settings. Yep, a lot of topics... My specialty is music reading. In that context the method works well and in my view for this reason: as one reads music and plays simultaneously, there is a continous input-output cycle going on. In music reading we don't need to guess whether one little note symbol was processed correctly or not - we hear it immeadiately! So this favourite topic of mine has proven, among all the study topics I have tampered with, a very useful application for eye tracking. I try nowadays to think of other research settings also from that perspective that I can link visual processing to some meaninful action. I also vary between explorative and more hypothesis-driven approaches, depending on the research question, but nowadays prefer the hypothesis-oriented one.",NA,"","general-discussion",164
"17",716935205385928832,"Zsofia Pilz#3944","03-Jun-20 09:44 AM","Okay then I will answer as well, even tough I am new to this feeld. I am especially interested in the field of text-reading and the emotions associated with reading. In the context of my bachelor's thesis, for example, I investigated how a text was perceived differently depending on which person in the text the test persons sympathized with. So the topic of *how we perceive a text depending on the emotions we feel towards the persons within the text* is quite interesting for me. In the context of my master thesis I am planning a similar experiment and suggestions are therefore of course very welcome.  
As a beginner in this field (and apparently not only beginners see it that way) I find it quite difficult to handle the collected data and to say which data should be used for the analysis and what exactly the measured data means.",NA,"","general-discussion",151
"18",716539063678795776,"Natalia Chitalkina#5818","03-Jun-20 10:28 AM","> Okay then I will answer as well, even tough I am new to this feeld. I am especially interested in the field of text-reading and the emotions associated with reading. In the context of my bachelor's thesis, for example, I investigated how a text was perceived differently depending on which person in the text the test persons sympathized with. So the topic of *how we perceive a text depending on the emotions we feel towards the persons within the text* is quite interesting for me. In the context of my master thesis I am planning a similar experiment and suggestions are therefore of course very welcome.  
> As a beginner in this field (and apparently not only beginners see it that way) I find it quite difficult to handle the collected data and to say which data should be used for the analysis and what exactly the measured data means.
@Zsofia Pilz  Our Turku colleague Johanna Kaakinen is also doing a similar kind of research on emotions associated with reading, I'm not sure if she has already published her findings. Maybe @Marjaana Puurtinen or our keynote speaker Raymond know more. Johanna and Raymond work in the same lab (led by J Hyon√§). Our colleage Henri Olkoniemi is doing research on sarcasm processing",NA,"","general-discussion",215
"19",716935205385928832,"Zsofia Pilz#3944","03-Jun-20 12:32 PM","Yes Johanna was kindly enough to involve me during my Erasmus in the team a little! I didn‚Äôt know about Henry‚Äôs research tough, thank you for the information @Natalia Chitalkina ! It sounds super interesting ‚ò∫Ô∏è",NA,"","general-discussion",38
"20",304446699206868992,"zombieowl#4403","03-Jun-20 02:00 PM","Thanks to all who have answered my first discussion question. It is time for another one! Feel free to keep answering them as we go per your convenience.",NA,"","general-discussion",28
"21",304446699206868992,"zombieowl#4403","03-Jun-20 02:01 PM","This next question is for @everyone
‚ùì Question: What is your go-to eye tracker for your particular use case?",NA,"","general-discussion",20
"23",304446699206868992,"zombieowl#4403","03-Jun-20 02:09 PM","I will answer the question on the go-to eye tracker. I like the Tobii X3-120 eye tracker for code comprehension studies. It is sleek and very mobile.  I find it much better in terms of accuracy and drift compared to the GazePoint.",NA,"","general-discussion",44
"24",521698252983238656,"OldCanada#6524","03-Jun-20 02:47 PM","Go-to eyetracker is Eyelink 1000.  Excellent technology, can be adapted for use with fMRI, EEG, etc.  Excellent technical and customer support for anything.  The one con for people doing more naturalistic experiments is that the system works best with a head and chin rest.  We also have Tobii glasses and an HTC Vive Virtual reality system with the same sensors built in for eyetracking in VR.  I'm excited about the prospects for both but haven't used either sufficiently to provide feedback",NA,"","general-discussion",84
"25",304446699206868992,"zombieowl#4403","01-Jun-20 08:27 PM","@Sarah Fakhoury has used fNIRs wtih eye tracking.  Very cool work! Can you tell us a little about it @Sarah Fakhoury?",NA,"üëç (2)","eye-tracking-plus-other-data-fmri-eeg-and-such",22
"26",600770279026982912,"peitek#2292","02-Jun-20 02:58 AM","@Zohreh Sharafi can also join in with their experiences on eyetracking+fMRI",NA,"üëç (2)","eye-tracking-plus-other-data-fmri-eeg-and-such",13
"27",716539063678795776,"Natalia Chitalkina#5818","02-Jun-20 05:34 AM","Regarding eye tracking vs fMRI I find two issues particularly interesting:  1)  that eye tracking and fMRI studies moved in parallel trying to study the same questions. So, the most popular research question was about whether experts read code and text in the same way. It all started from looking for similarities, but later it turned out that there are also some important differences (code eye tracking studies: less linear processing-although the eye movement data is contradictory, more regressions for code compared to text). Although first fMRI studies showed that code reading is linked to the language processing centers,  one of the last studies stated that language-sensitive areas of the brain are differentially recruited when processing code versus English prose.     The paper also includes a nice review of one NIRS code reading study. 2) Despite trying to study the same research questions it seems to me that there are different trends in the development of methods. In fMRI, the current trend is towards the standardization of the  preprocessing of the data. The most confusing moment of eye tracking research for me is almost a total absence of standardization in the preprocessing: a wide variety of eye movement classification algorithms, no standard way of calculation of pupil dilation etc. Even if I-VT is a kind of default, you still have to choose the threshold.",NA,"üëç (3)","eye-tracking-plus-other-data-fmri-eeg-and-such",226
"28",716509145263636480,"Zohreh Sharafi#1998","02-Jun-20 08:53 AM","In my experience, I found the limitations imposed by fMRI on the  ecological validity  of the experiment quite extensive.  By no way, lying down on a magnetic tube could mimic the real environment in which  developers work. When we design the experiment, each stimulus needs to be shown for a quick period of time (normally 30s to 1 min) which is also  unrealistic.  Also, conducting a fMRI study is quite expensive, compared to other means such as eye-tracking. Yet, it is also quite rewarding. The brain  imaging can reveal information that are not  accessible by other tools  which make them quite appealing.",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",103
"29",716509145263636480,"Zohreh Sharafi#1998","02-Jun-20 08:57 AM","@Natalia Chitalkina you mentioned some very interesting points. I also found analyzing the fMRI data quite different from let's say eye-tracking ones. I think it is a mature, standard process. There are a couple of well-known methods with a series of standard process that every ones follows which is very nice. Still, you need to design the experiment correctly and interpret the results carefully, but that standard process (based on rigorous mathematical/statistical methods) makes it reliable and easier to follow. Maybe, this maturity comes from years of experience coming from medical studies which the price to pay for making mistakes are quite high.",NA,"üëç (2)","eye-tracking-plus-other-data-fmri-eeg-and-such",108
"30",717132069133615232,"Sarah Fakhoury#8253","02-Jun-20 11:01 AM","A lot of the points you brought up  @Zohreh Sharafi for fMRI data also applies for fNIR. However, it seems that procedures for the analysis of fMRI data is more standard than for fNIR.  From what I have seen for fNIR, there are several different analysis approaches and pre-processing procedures that are acceptable, instead of a single gold standard. Different approaches provide different benefits for removing signal/physiological noise, and there is no consensus on which approach applies to all experimental senarios. This paper does a good job at explaining this: 

This is a challenege, especially when it comes to integrating eyetracking data with fNIR data. For example, when temporally synchronizing fNIR data with eyetracking gazes, the inherent hemodynamic latency (around 5-7 seconds) for the fNIR data can significantly vary where gazes are mapped. Most papers I have come across use a fixed number for delay across all participants, however I think it is worth looking into how we can more accurately model individual latency.",NA,"üëç (4)","eye-tracking-plus-other-data-fmri-eeg-and-such",167
"31",696595882325442688,"Takatomi Kubo#1749","02-Jun-20 11:29 AM","@Natalia Chitalkina 
> Although first fMRI studies showed that code reading is linked to the language processing centers,  one of the last studies stated that language-sensitive areas of the brain are differentially recruited when processing code versus English prose. 

FYI.
Comprehension of computer code relies primarily on domain-general
executive resources
Anna A. Ivanova et al.

>  We found that the MD system exhibited strong bilateral responses to code in
> both experiments. In contrast, the language system responded strongly to sentence problems, but only
> weakly or not at all to code problems. We conclude that code comprehension relies primarily on domain general executive resources, demonstrating that the MD system supports the use of novel cognitive tools
> even when the input is structurally similar to natural language.",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",126
"32",716539063678795776,"Natalia Chitalkina#5818","02-Jun-20 01:53 PM","@Takatomi Kubo Thank you! This is a very interesting fresh paper that I haven't seen. We should take findings into account for our own studies. I really like the setup.  I wonder whether there is a shift in involved brain regions with the development of programming skills in some way (for example, a shift was found for Arabic symbolic number processing in children of different age). It would be really interesting to see the brain activation of beginners  during the same tasks",NA,"üëç (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",84
"33",696595882325442688,"Takatomi Kubo#1749","02-Jun-20 02:33 PM","@Natalia Chitalkina 
It is very good to hear that 
you have interest in the paper.
Our work shows the language system is involved in program comprehension.
We used brain decoding approach, and assessed correlation between  the decoding accuracies of brain regions and behavioral performance that can reflect a kind of expertise level.


I hope you will have an interest in our work as well.",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",65
"34",476623791569633280,"imnotamember#0100","02-Jun-20 03:26 PM","Hey all, interesting discussion going on in here! Just to add to it, I've been doing primarily EEG and fMRI for the past few years in a neuroscience program and heavily invested in the different methods applied to each. I recently was lucky enough to help on a massive paper comparing how different groups of neuroscientists analyze the same fMRI dataset to answer predetermined-hypotheses ( or ) and long-story short, everyone analyzes differently (no gold standard) and it can lead to variability in interpretation of the results. Fortunately, there was a majority of agreement in interpretation even with varying analysis methods. The real standard is to have converging evidence amongst many groups and not just let one group dictate a finding.",NA,"üëç (2)","eye-tracking-plus-other-data-fmri-eeg-and-such",122
"35",476623791569633280,"imnotamember#0100","02-Jun-20 03:30 PM","EEG is even more variable as most of the recording equipment comes with proprietary software for analysis, and even if you export for analysis in 3rd party software (EEGLAB, MNE Python, FieldTrip, ERP Lab, etc.), there's a massive range of analysis techniques employed.",NA,"üëç (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",44
"36",476623791569633280,"imnotamember#0100","02-Jun-20 03:31 PM","In many ways I'd say eye-tracking is less varying in analyses compared to EEG, and on par with fMRI. Again, the real standard is converging evidence amongst many groups and using the right method for the right question.",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",40
"37",600770279026982912,"peitek#2292","03-Jun-20 02:31 AM","I agree with Josh. As a member of one of the groups that use fMRI, our analysis differs significantly from what some of the other groups are doing. There is no single gold standard, but there are a handful of analysis procedures that each are accepted if you stay within certain limits. In my view this is the key difference to eye tracking, where there is neither a single or a handful of accepted standards. In my few submitted papers, my fMRI analyses were never fundamentally questioned, but the eye-tracking analysis was even if it mirrored a previous paper we were replicating.

I already mentioned that in the #general-discussion channel, but I wish there was a methodological framework that researchers could rely and refer to when analyzing their eye-tracking data. I don't see why eye-tracking cannot have one or a handful of robust standards.",NA,"üíØ (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",149
"38",600770279026982912,"peitek#2292","03-Jun-20 02:35 AM","> @Natalia Chitalkina 
> 
> FYI.
> Comprehension of computer code relies primarily on domain-general
> executive resources
> Anna A. Ivanova et al.
> 

This is a really interesting paper, but one needs to be quite careful when interpreting fMRI data. The authors contrast brain activation when reading code and reading text and conclude that code has little involvement of the language centers. But, to my understanding, the analysis shows that reading code just has less involvement of language centers than when reading text. So the language centers still may be quite involved during code comprehension, it's just not visible with the chosen contrast of reading text (which naturally will involve language centers).",NA,"üëç (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",110
"39",600770279026982912,"peitek#2292","03-Jun-20 02:38 AM","> A lot of the points you brought up  @Zohreh Sharafi for fMRI data also applies for fNIR. However, it seems that procedures for the analysis of fMRI data is more standard than for fNIR.  From what I have seen for fNIR, there are several different analysis approaches and pre-processing procedures that are acceptable, instead of a single gold standard. Different approaches provide different benefits for removing signal/physiological noise, and there is no consensus on which approach applies to all experimental senarios. This paper does a good job at explaining this: 
> 
> This is a challenege, especially when it comes to integrating eyetracking data with fNIR data. For example, when temporally synchronizing fNIR data with eyetracking gazes, the inherent hemodynamic latency (around 5-7 seconds) for the fNIR data can significantly vary where gazes are mapped. Most papers I have come across use a fixed number for delay across all participants, however I think it is worth looking into how we can more accurately model individual latency.

To bring it back to the original topic of neuro+eyetracking, I encountered the same challenges as @Sarah Fakhoury. When trying to align eye-tracking with fNIR/fMRI, the differences in temporal resolution and delays make it imprecise. We still have this in our agenda, since the potential benefit from a combination of the modalities is huge, but it's not an easy problem to solve üòÖ",NA,"üíØ (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",235
"40",716539063678795776,"Natalia Chitalkina#5818","03-Jun-20 07:24 AM","> @Natalia Chitalkina 
> It is very good to hear that 
> you have interest in the paper.
> Our work shows the language system is involved in program comprehension.
> We used brain decoding approach, and assessed correlation between  the decoding accuracies of brain regions and behavioral performance that can reflect a kind of expertise level.
> 
> 
> I hope you will have an interest in our work as well.
@Takatomi Kubo Thank you, I read this paper with a great interest! It was probably the first fMRI paper I saw with participants of three different expertise levels.  And I find it interesting that the language system is involved in program classification into categories task. Your study and Ivanova's and al are probably about different kinds (or levels) of code comprehension: understanding the idea of codes and understanding of outputs. Both are very interesting, but for the first,  participants didn't necessary have to trace the whole code (rather a kind of expertise based visual search for cues), for the second, probably more details should be taken into account by participants. It's also interesting to notice that the vast majority of Ivanova's sample was female experts and the second study didn't have expert females.",NA,"üëç (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",203
"41",716539063678795776,"Natalia Chitalkina#5818","03-Jun-20 07:44 AM","> This is a really interesting paper, but one needs to be quite careful when interpreting fMRI data. The authors contrast brain activation when reading code and reading text and conclude that code has little involvement of the language centers. But, to my understanding, the analysis shows that reading code just has less involvement of language centers than when reading text. So the language centers still may be quite involved during code comprehension, it's just not visible with the chosen contrast of reading text (which naturally will involve language centers).
@peitek This is a very good point!  Of course, there are many systems involved in code comprehension, since it's a pretty complicated activity. And it's possible to find them with fMRI. The other important thing is that these systems are involved with different contribution at different levels of expertise development (this works for different cognitive processes like writing with a pen or Arabic number symbol processing according to neuropsychological research, so I might assume it should work for programming too). I like the setup of Ivanova's study, because it allowed to look at which system takes the lead in this kind of code comprehension tasks in experts. They would see nothing, if code and text reading were similar in terms of brain processing of the tasks. It's interesting to find out if that's the case for beginners, it would be in line with eye tracking findings of linear reading of text and code in beginners. That's the shift I meant in my previous message",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",261
"42",716509145263636480,"Zohreh Sharafi#1998","03-Jun-20 08:53 AM","This is very interesting discussion. 
I agree that there is no gold standard but I found it more mature that eye-tracking one. 
My understanding is that every fMRI analysis starts with a pre-processing phase in which a set of standard approaches are generally used for that such as computing voxel displacement maps (VDMs), segmenting anatomical scans, and  transform them based on a standard template such as MNI one.
Then, there is this First-level analysis to first generate first-level model and then combine them in groups.  
Then there is Contrasts and group-level analysis. Also, there are other types of analysis such as connectivity analysis, that are inherently different. 

I understand that these procedures need to be tuned for each experiment based on the design and the number of factors involved. Also, how to interpret the results is a complex creative process. But still I found it more standard than eye-tracking analysis, specially while talking to researchers in biomedical engineering or medical imaging research for example, all of them almost follow the same approaches, maybe using 
various stats.",NA,"üëç (1),üî¨ (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",182
"43",696595882325442688,"Takatomi Kubo#1749","03-Jun-20 08:59 AM","@Natalia Chitalkina 
Thank you for your reply!
>  The other important thing is that these systems are involved with different contribution at different levels of expertise development (this works for different cognitive processes like writing with a pen or Arabic number symbol processing according to neuropsychological research, so I might assume it should work for programming too).

We have to control many factors, including sex differences, dominance of hand, etc., due to  the different response patterns in brain activities, even for fNIRS.
In addition, in our study, we randomized button assignment to answer,
and designed visual stimuli carefully as much as possible.
We checked S/N ratio (here, signal means response for difference of categories of codes) with changing setting of experiments.",NA,"üî¨ (1),ü§© (1),üëç (1)","eye-tracking-plus-other-data-fmri-eeg-and-such",122
"44",476623791569633280,"imnotamember#0100","03-Jun-20 09:02 AM","@Zohreh Sharafi you got it right regarding many of the methodological practices. One of the harder parts to learn when starting in fMRI research is that we really need to scan larger numbers of participants (upwards of 100 for simple studies) to limit our spurious findings and there's no standard for contrast AOI's (whole brain contrasts are very imprecise). Then there's the issue of how many trials are acceptable to remove from a single participants' data before removing them entirely from group-analysis (10%, 25%, 3 trials?).",NA,"üëç (2)","eye-tracking-plus-other-data-fmri-eeg-and-such",91
"45",476623791569633280,"imnotamember#0100","03-Jun-20 09:04 AM","On top of that, fMRI is really expensive, so most studies will either skip dropping bad participant data or use methods to process them into better data (which introduces new biases)",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",31
"46",716509145263636480,"Zohreh Sharafi#1998","03-Jun-20 09:05 AM","@imnotamember I agree 100%. Even the complex analysis in addition to the actual cost of conducting the experiment make fMRI much more expensive that eye-tracking for example.",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",29
"47",476623791569633280,"imnotamember#0100","03-Jun-20 09:10 AM","@Zohreh Sharafi What this does provide is a great opportunity in the eyetracking community to learn from these issues. We could do tons of relatively cheap eyetracking data collection amongst many groups around the world and attempt to see how different standards change results!",NA,"‚ù§Ô∏è (1),üëç (2)","eye-tracking-plus-other-data-fmri-eeg-and-such",45
"48",476623791569633280,"imnotamember#0100","03-Jun-20 09:11 AM","Might be a useful project we could organize here and return to next year!",NA,"","eye-tracking-plus-other-data-fmri-eeg-and-such",14
"49",476623791569633280,"imnotamember#0100","03-Jun-20 09:23 AM","I gotta say I'm so impressed by everyone in this channel coming to share and learn. If we were at a neuroscience conference these discussions would devolve into heated (and useless) debates rather quickly. üòç ü§© ü§ì",NA,"üòÜ (1),ü§£ (2)","eye-tracking-plus-other-data-fmri-eeg-and-such",38
"50",716509145263636480,"Zohreh Sharafi#1998","03-Jun-20 09:24 AM","> @Zohreh Sharafi What this does provide is a great opportunity in the eyetracking community to learn from these issues. We could do tons of relatively cheap eyetracking data collection amongst many groups around the world and attempt to see how different standards change results!
@imnotamember Excellent idea, as a community, we have all the resources to make this happen.",NA,"üíØ (2),‚ù§Ô∏è (3)","eye-tracking-plus-other-data-fmri-eeg-and-such",60
"51",476623791569633280,"imnotamember#0100","03-Jun-20 09:30 AM","One thing I find in the general neuroimaging community is it frequently is isolated groups of biologists/psychologists/computational modelers/etc., each doing the same study different ways instead of combining their expertise to really approach each step thoughtfully.",NA,"üëç (3)","eye-tracking-plus-other-data-fmri-eeg-and-such",39
"52",716539063678795776,"Natalia Chitalkina#5818","31-May-20 05:29 AM","Dear all, here is our preprint","https://cdn.discordapp.com/attachments/707137061664063530/716599188929904640/Chitalkina_et_al.pdf","üëç (3)","proof-readers-error",6
"53",716539063678795776,"Natalia Chitalkina#5818","31-May-20 05:45 AM","And this is my presentation. I hope that you find it interesting. Any comments are very welcome!  ","","üëç (7)","proof-readers-error",17
"54",304446699206868992,"zombieowl#4403","01-Jun-20 09:27 AM","Moderators for this channel are @zombieowl and @Marjaana Puurtinen","","","proof-readers-error",9
"56",716509145263636480,"Zohreh Sharafi#1998","01-Jun-20 03:24 PM","@Natalia Chitalkina thanks for the presentation. Your work has a very interesting and novel research question. The results are also surprising. Nice work. I'm wondering if you had any after experiment follow-up with the participant. Like as an expert, what the participant thinks of the results?","","","proof-readers-error",49
"57",716539063678795776,"Natalia Chitalkina#5818","02-Jun-20 04:17 AM","@Zohreh Sharafi thanks for your comments and interest in my work! Yes, I showed to the participant the scanpath video with his eye movements (he was a CS, but non eye tracking expert) right after the study and asked how he felt during the study and what he thought about it. Of course, I didn't have the results of eye tracking metrics at that moment. He was a bit confused that he made mistakes in the tasks, but I assured him that we are very interested in all kind of experiences people have in this study including mistakes, because it might help to find out how prediction in code reading works.","","üëç (1)","proof-readers-error",113
"58",521698252983238656,"OldCanada#6524","02-Jun-20 11:39 AM","HI @Zohreh Sharafi , nice job with this.  Apologies if I missed this in the presentation but can you say a little more about how/why you chose the eye movement metrics you did?  Is it the case that you looked at a lot of different metrics and are reporting only significant differences or was there a specific reason you selected the ones you did?  I think there's a lot of value to an approach like this but it is also very useful to have some sense of why certain metrics may be more or less useful.  Also, are you planning to apply this approach to future subjects or was this more of an initial determination of how you'd use the eyetracker in this domain?  Thanks!","","","proof-readers-error",127
"59",600770279026982912,"peitek#2292","03-Jun-20 07:15 AM","@Natalia Chitalkina Thank you for the interesting talk! 

Did I understand it correctly that you told the participant *after* the experiment which snippets he got correct/incorrect? If so, one option in the future you could consider is giving this feedback after each task. In my experience if an expert hears that they made a mistake they'll try even harder on the next attempt. But such feedback-loop has an influence on the behavior as well, so it's a dangerous choice..","","","proof-readers-error",84
"60",716509145263636480,"Zohreh Sharafi#1998","03-Jun-20 08:30 AM","@OldCanada, Hi, i think those questions are for Natalia, I haven't been involved in this research work.","","","proof-readers-error",19
"61",521698252983238656,"OldCanada#6524","03-Jun-20 09:07 AM","My apologies, I was accidentally looking at your comment above rather than the author in asking my question (sorry for the mixup too @Natalia Chitalkina","","","proof-readers-error",25
"62",716539063678795776,"Natalia Chitalkina#5818","03-Jun-20 09:56 AM","Dear @OldCanada, the choice of metrics in this study was defined by the research question and the nature of the task: we wanted to look at whether proof-readers' errors require rechecking (number of fixations, number of visits) and  a different fixation time (fixation duration) compared to originals and whether they required more cognitive effort (saccadic peak velocity and average velocity of the first saccade). As I said in the presentation, I was looking for an alternative to pupil dilation, because the change in pupil dilation occurs with a delay, so in this task I have to analyse it in fixations after the altered element, but they might be done to different parts of code in originals and variations that makes the analysis more challengable (due to the PFE) . My idea was to pilot saccadic parameters to look at whether and when they will work in the task. My interest to the saccadic peak velocity was not random: one of the co-authors of DiStasi 2010 work that I refer to is an eye movement expert and my first cognitive psychology teacher (psychologist and physicist B Velichkovsky). I plan to continue to apply this approach (with the choice of metrics defined by research questions and the nature of the task). This was a side project to our main study with a sample of participants on how experts deal with unexpected changes in different parts of a familiar pseudocode","","","proof-readers-error",238
"63",716539063678795776,"Natalia Chitalkina#5818","03-Jun-20 10:07 AM","@peitek thank you for watching! Yes, the participant didn't know whether he was correct. This is an interesting new idea I will keep in mind for future studies! For the current study, we need to keep participants uninformed, since we are also interested in cognitive load and it might be affected by almost everything including motivation, whether we communicate with participants during the tasks, whether participants thought aloud etc. The current participant thought aloud despite we haven't asked him to do so, so I decided to make a side case study project out of his data","","","proof-readers-error",99
"64",521698252983238656,"OldCanada#6524","03-Jun-20 10:31 AM","Thanks, and yes, avoid pupil dilation without a) a baseline to account for time as you suggest, and b) without a stationary eyetracker with participant in head and chinrest.  Changes in lighting, movement of the participants (which changes the angle of the camera relative to the eye) etc. can all impact that.  Thanks for the additional clarifications","","","proof-readers-error",57
"65",610317742020624384,"Naser#1609","03-Jun-20 01:06 PM","Hello @Natalia Chitalkina, thank you for this interesting and informative presentation.

I think single subject studies are interesting as they offer rich data that can be studied in depth, and they are the dominant form of studies in cognitive neuropsychology.  I am interested in knowing if you think there is a role for the length of the target token in how likely it is to be skipped by the eyes?

Also, what are your thoughts on the meaningful unit in code? do you think a code chunk is a possible candidate?
Thanks again for a thought provoking presentation!","","","proof-readers-error",98
"66",716659329717239936,"unaizah#9120","31-May-20 09:49 AM","Hi all! üëã  Thanks for visiting this channel.","","","novice-programmers-gaze-patterns",8
"67",716659329717239936,"unaizah#9120","31-May-20 09:57 AM","Here's a copy of our paper","https://cdn.discordapp.com/attachments/707137161044033546/716666600584904784/EMIP2020_ID62.pdf","üëç (4)","novice-programmers-gaze-patterns",7
"68",716659329717239936,"unaizah#9120","31-May-20 10:28 AM","and here's the presentation ..we welcome any comments, suggestions or feedback.. üôÇ    ","","üëç (6)","novice-programmers-gaze-patterns",13
"69",716935205385928832,"Zsofia Pilz#3944","01-Jun-20 07:55 AM","Very interesting presentation! 
Regarding future work, I would find it very interesting to see if there are differences in people with different backgrounds of programming fields.  For example, if someone learning linguistic computing, web developer or computational mathematics perceives the code differently than someone studying traditional computing.","","üëç (1)","novice-programmers-gaze-patterns",47
"70",716539063678795776,"Natalia Chitalkina#5818","01-Jun-20 08:16 AM","Hi, Unaizah! Thanks for a very interesting presentation! I thought that maybe there was no consistency in gaze patterns prior to the final answer, because some of participants in some tasks took some time to relax before the next task, and it affected their eye movement pattern... It would be interesting to add to the study (or your future studies on the same topic) any kind of a simple working memory test (such as digit span) to look at whether there is actually a relation between WM capacity and performance of these tasks at different levels of difficulty. I found it very interesting that difficult tasks required more regressions. In my current study (a case study presented by me here is a side project of it), I had the same number of lines in pseudocode, I got the idea to check  if pseudocodes with alterations (more difficult) get more regressions. And I'm really waiting to hear how gender affects performance in these tasks! You are doing a great job! This year I realized how difficult is to recruit females for code studies:)","","‚ù§Ô∏è (1)","novice-programmers-gaze-patterns",183
"71",304446699206868992,"zombieowl#4403","01-Jun-20 09:48 AM","Moderators for this channel are @peitek and @zombieowl","","","novice-programmers-gaze-patterns",8
"73",716659329717239936,"unaizah#9120","01-Jun-20 10:22 AM","Hi @Zsofia Pilz ! Thanks for your input! Interesting idea, thank you! üòÉ In fact, adding to your comment, given the current IR4.0 era that requires many (include non technical people) to  comprehend technical programming skills, I think it's important to identify how these problem solvers mentally represent and strategize their solutions. The findings could eventually support personalized learning.","","","novice-programmers-gaze-patterns",60
"74",716659329717239936,"unaizah#9120","01-Jun-20 10:26 AM","Hi Natalia! I hope you‚Äôre well! It‚Äôs so nice to catch up with you again ‚Äì virtually. üë©‚Äçüíª 
Yes, true enough, there could be simply many reasons for not finding coherent patterns prior to the final answer. Idiosyncrasies among the participants gaze patterns or even other psychological factors such as interest, motivation, confidence, etc could be some of the driving factors. I like your idea on assessing workload capacity, I think this will be an added verification for the regression pattern and on their validation strategies. I‚Äôm glad that you found some of the current findings useful for your work. Thank you for your encouragement! May your work progress well in the coming months! Girl power! ü¶∏‚Äç‚ôÄÔ∏è Keep in touch!","","","novice-programmers-gaze-patterns",128
"75",716659329717239936,"unaizah#9120","01-Jun-20 10:27 AM","> Hi @Natalia Chitalkina ! I hope you‚Äôre well! It‚Äôs so nice to catch up with you again ‚Äì virtually. üë©‚Äçüíª 
> Yes, true enough, there could be simply many reasons for not finding coherent patterns prior to the final answer. Idiosyncrasies among the participants gaze patterns or even other psychological factors such as interest, motivation, confidence, etc could be some of the driving factors. I like your idea on assessing workload capacity, I think this will be an added verification for the regression pattern and on their validation strategies. I‚Äôm glad that you found some of the current findings useful for your work. Thank you for your encouragement! May your work progress well in the coming months! Girl power! ü¶∏‚Äç‚ôÄÔ∏è Keep in touch!
@unaizah","","‚ù§Ô∏è (1)","novice-programmers-gaze-patterns",131
"76",716539063678795776,"Natalia Chitalkina#5818","01-Jun-20 12:07 PM","Thank you, @unaizah ! I'm glad to see the progress of your work! Good luck with the continuation of your analysis! Keep in touch!","","üòò (1)","novice-programmers-gaze-patterns",24
"77",716509145263636480,"Zohreh Sharafi#1998","01-Jun-20 05:14 PM","@unaizah  thanks for the great presentation. The work is also quite interesting. Can you please tell us more about the data mining approach? I'm interested to know more about the input (was it the raw fixation data or like a sequence of AOI-based scanpath) and the output of the algorithm?","","","novice-programmers-gaze-patterns",53
"78",600770279026982912,"peitek#2292","02-Jun-20 02:49 AM","Hi @unaizah also a thank you from me for an interesting presentation! I have a question regarding the cluster analysis. Does the amount of data influence the clustering? Assuming participants take longer to solve the hard problems, the cluster algorithm would have more data for each participant to work with, which may make it possible to group the hard tasks to fewer clusters?","","üëç (1)","novice-programmers-gaze-patterns",63
"79",715584647530020864,"Teresa#9355","02-Jun-20 11:14 AM","Thanks for the presentation Unaizah!
Out of curiosity, were there other approaches to pattern analysis you considered, but dismissed?","","üëç (1)","novice-programmers-gaze-patterns",19
"80",521698252983238656,"OldCanada#6524","02-Jun-20 11:44 AM","Thank you for your presentation @unaizah .  Do you have any other data collected on the programmers themselves (e.g. individual difference variables, personality variables, working memory capacity)? I think these are the types of additional variables that might afford you some insight beyond programming experience (there may be clear differences in how people approach programming or programming issues that link to these individual differences)?","","üëç (1)","novice-programmers-gaze-patterns",64
"81",707150407104724992,"Marjaana Puurtinen#3084","03-Jun-20 01:59 AM","Thank you, @unaizah ! As it happens, I was just doing (last week) simple scan path analyses for one of my data sets (participants viewed a salad buffet for with 14 dishes for 20 s prior to food selection), so these types of analyses are fresh in my mind and I checked your regression and cluster analyses with great interest. üôÇ First, because your participants viewed the tasks for non-defined periods of time, I would check regression etc. as % of all shifts, and not use the absolute counts (or do both, just to be sure). I myself calculated two scan path (or transition) parameters that I thought might help to identify the participants' viewing profiles. The AOIs were numbered in consecutive order. One parameter was amount of shifts from AOI X the ""next"" or ""previous"" AOI (as % of all shifts between AOIs), this to describe a ""stepwise"" gazing pattern. The other was the amount of shifts when AOI X was fixated AND returned to after visits to 1-3 other AOIs (as % of all shifts), this to describe participants' need/will to return to selected dishes instead of moving forward (the limit 1-3 was set based on data).  In my case, the first one was useful, but in your case, this ""return"" behaviour might be even more revealing (e.g. in text reading research Hy√∂n√§ et al. have reported different reading profiles that were also associated with different types of task performance, and there, too, look back were one of the meaninful differentiating characteristics). Some more ideas: task performance is good to check, that might helpt to explain the clusters or help in defining new onew. In educational studies sometimes participants are asked to indicate how sure they are of each of their answer, when doing the task. If you plan a new data collection and still search for code-reading patterns, this self-rated certainty might help to explain the gaze patterns, or be included as one factor for grouping the data. All the best for your work!","","üíØ (1)","novice-programmers-gaze-patterns",341
"82",716659329717239936,"unaizah#9120","03-Jun-20 07:32 AM","> @unaizah  thanks for the great presentation. The work is also quite interesting. Can you please tell us more about the data mining approach? I'm interested to know more about the input (was it the raw fixation data or like a sequence of AOI-based scanpath) and the output of the algorithm?
Hi @Zohreh Sharafi. It‚Äôs nice to finally speak with you. üòÉ Your research work has always inspired me. Yes, we started with the raw fixation data from Tobii project file and pre-process the AoI Hit column to produce the needed AoI sequences into subsets for respectively the first click set and the last click set. We do this for every question. The lists of the corresponding AoIs according to the set of interest ‚Äì 3,4,5 for all participants were used as the input files. The rule association output generates (if any) list of AoIs with high degree of associations for each left (x) and right (y) sides - (akin to the if x, then y rules) in the format of {x} => {y} followed by several measures such as support, confidence and lift. The strength of this relationship (left AoIs vs right AOIs) will appear as the probability given by the rule‚Äôs confidence. Support means how many participants‚Äô satisfy (support) this condition, while lift gives the correlation between x and y (to what extent AoIs_x affects AoIs_y).
I hope this is useful. @Drew, feel free to add more details on this in case if I missed any important points.","","","novice-programmers-gaze-patterns",255
"83",716659329717239936,"unaizah#9120","03-Jun-20 07:36 AM","> Hi @unaizah also a thank you from me for an interesting presentation! I have a question regarding the cluster analysis. Does the amount of data influence the clustering? Assuming participants take longer to solve the hard problems, the cluster algorithm would have more data for each participant to work with, which may make it possible to group the hard tasks to fewer clusters?
Hi @peitek! Thank you for your interest and question. Yes, I would think that the amount of data (breadth=more participants and depth=detailed AoI sequences) will influence the clustering. As of now, in the analysis reported, we only considered the subsets of the AoIs (last few AoIs visited before the mouse click event) because we‚Äôre interested to identify the patterns of validation strategy exhibited by the participants ‚Äì a snapshot of the viewing pattern. I suspect (I might be wrong! and this needs further investigation/assessment), if we consider more participants (breadth) grouped by expertise/years of experience, we could potentially get a set of more robust/representative clusters due to the differences by performance (for example).  However, considering more AOI sequences for each participant (depth) may potentially produce more variances in terms of the gaze patterns (under the assumption that any visual behavior cannot be controlled strictly by some definite plan). Due to this, identifying a representative gaze pattern via sequence similarity within- and between- groups can be tricky. Good news is, there are already a number of available methods in the literature that can be adopted for such comparisons that considers both spatial and temporal aspects of the data.  One possible way to test is to have participants do as many tasks for each complexity, but the next question to consider is on the problem solving strategy for each problem that can be different in its own way although they are at the same level of complexity. This is worth exploring,I think.","","","novice-programmers-gaze-patterns",321
"84",716659329717239936,"unaizah#9120","03-Jun-20 07:43 AM","> Thanks for the presentation Unaizah!
> Out of curiosity, were there other approaches to pattern analysis you considered, but dismissed?
Hi @Teresa!  Thanks for watching! You work too always interest me. Nice to meet you! 
Yes, on top of the reported apriori and HCA, we briefly investigated the relationship of the clusters by doing crosstabs (chi square tests) with the current demographics data (gender, year of study, language competency, etc). As of now, the results returned non-statistically significant relationship interpreted as no effect from these parameters on the problems. But this could be due to the disproportion distribution of our data ‚Äì some groups contain fewer data than the others (i.e. participants in specific year of study). 
Additionally, there are a number of other ways to compare scanpaths some of which are adopted from the genes/DNA analysis methods apart from the Levenshtein (string-edit) approach. These are worth looking into. 
If you‚Äôve any ideas of how the data can be further explored, we‚Äôre more than happy to ‚Äòlisten‚Äô and consider. We‚Äôre also open for collaborations.","","","novice-programmers-gaze-patterns",182
"85",716659329717239936,"unaizah#9120","03-Jun-20 08:01 AM","> Thank you for your presentation @unaizah .  Do you have any other data collected on the programmers themselves (e.g. individual difference variables, personality variables, working memory capacity)? I think these are the types of additional variables that might afford you some insight beyond programming experience (there may be clear differences in how people approach programming or programming issues that link to these individual differences)?
Thank you for watching @OldCanada ! We have a number of other data mainly on demographics such as gender, year of study, English language proficiency level and programming experience (grades scored in previous programming courses, years of experience, overall expertise rating, frequency of writing programming codes and use of pseudocodes) collected via post-study survey. We‚Äôre still thinking about how to make use these survey data along the eye-tracking findings. Data for the study reported in this paper was collected some years ago, unfortunately, for some reason, we didn‚Äôt collect data on confidence/difficulty rating or working memory. But yes, I agree that data related to working memory would be an added value and will give more depth to the findings. Thank you for the suggestions!","","üëç (1)","novice-programmers-gaze-patterns",193
"86",716659329717239936,"unaizah#9120","03-Jun-20 08:09 AM","Hi @Marjaana Puurtinen!! Thank you for sharing your experience and recommendations on how the analysis can be expanded. üíù  I'm learning so much! yes, i'll check on these and yes,we've learned from mistakes on experimental design from previous experience, the self-rate certainty is something we're now doing in more recent data collection. Thank you for your support!! üòÄ","","üôÇ (1)","novice-programmers-gaze-patterns",64
"87",610317742020624384,"Naser#1609","03-Jun-20 12:28 PM","Hello @unaizah very interesting talk, I've learned a lot from it.

I am wondering if the HCA clusters correlate to skill level, I would be interested in hearing your thoughts on that. Also, do you think that randomizing the location of the next question button might have some effect on the gaze pattern before the click.","","","novice-programmers-gaze-patterns",57
"88",610317742020624384,"Naser#1609","19-May-20 01:48 PM","Read paper: ",NA,"üëçüèΩ (3)","e-z-reader-model-prediction",2
"89",610317742020624384,"Naser#1609","31-May-20 11:16 AM","Here is the presentation: 
Looking forward to reading your questions and comments!",NA,"üëçüèΩ (3),üëç (1)","e-z-reader-model-prediction",12
"90",304446699206868992,"zombieowl#4403","01-Jun-20 09:49 AM","Moderators for this channel are @peitek and @Marjaana Puurtinen",NA,"","e-z-reader-model-prediction",9
"92",600770279026982912,"peitek#2292","02-Jun-20 01:57 AM","Hi @Naser, thank you for exciting talk. I usually take notes and screenshots for tweeting about each talk later on, but I was so focused on listening to your talk I forgot about it üòÜ 

I'm a fan of bringing in theories and models from other disciplines and the e-z model is a good candidate to be useful for software engineering. Nice work!

I have a couple of clarification questions:
- In your word frequency analysis, did you consider that code often uses a lot of compound identifiers? Let's say ""addNumbers"" by itself is a long and probably rare word (= prediction should be difficult to process), but consists of two short and common components. As a native speaker of German, compound words is a common issue with natural language models.
- Does the E-Z reader model consider whether it's the readers' native language? If I remember correctly, your analyzed data from @Teresa were on non-native speakers, which could explain to a degree that the model was over-optimistic with its predictions.

A couple of questions for future directions:
- I'm wondering if programmers build up a ""local"" dictionary of common words in addition to a global one. Let's say I'm a Java programmer that works in neuroscience. Words that are globally common like string, int, float etc will be processed quickly. But, because I work in the context of neuroscience, I may also be able to process words quickly that are not globally common (""hemisphere""). In a way, domain knowledge may be a refining factor for a source code adaption of the e-z reader model?",NA,"","e-z-reader-model-prediction",272
"93",600770279026982912,"peitek#2292","02-Jun-20 01:57 AM","- One concern I have with the ""reading"" model is that it may miss the layer of comprehension. For example, the e-z model would suggest to use short (and common) identifiers to ease reading. While this is sensible, to actually *comprehend* the code, longer identifiers may be helpful. Essentially, longer identifiers may make the reading part slower, but the comprehension part easier (e.g.,  )?",NA,"","e-z-reader-model-prediction",65
"94",610317742020624384,"Naser#1609","02-Jun-20 07:02 AM","Hello @peitek, thank you for your kind comments and important questions.

In regards to your first question, in the presented work we did not split compound tokens, but this idea is definitely interesting and calls for further contemplation in future work.

To answer your second question, counting token/word frequencies from large corpora of text is intended to estimate how much exposure subjects have had to the language, with subjects who are not native speakers of the language this estimate is more difficult to calculate.  Nonetheless, since the ""over-optimistic"" predictions appeared in natural text and source code I would imagine that this issue can be solved by fine tuning the model parameters to approximate observed data better.",NA,"","e-z-reader-model-prediction",118
"95",610317742020624384,"Naser#1609","02-Jun-20 07:03 AM","Regarding your third question, I think you bring up an excellent point on domain knowledge.  This can also appear on the comprehension level where the word ""bank"" would mean something different to a financier than a person who is concerned with rivers.

To address your last concern, the E-Z reader accounts to some level for comprehension with predictability as a place holder in the word-identification system.  This can be observed when a predictable word is identified through comprehension before the eyes move to it (top-down processing).  An example of that would be: The doctor told Fred that his drinking would damage his ______. Of course, the missing word is ""liver"", which could receive a very short or no fixation and still be identified through top-down processing.   

At the same time, E-Z reader does not explain how word integration or post-lexical processing happen.  In a previous paper I made a modification to the E-Z reader model to generate semantic networks of comprehension with natural text that you might find interesting (",NA,"","e-z-reader-model-prediction",176
"96",610317742020624384,"Naser#1609","02-Jun-20 07:03 AM","Finally, regarding the paper that you kindly shared, the E-Z reader model agrees with the findings of that paper.  E-Z reader also explains why identifier names that are dictionary words facilitate reading, as subjects have been exposed to dictionary words more than single letters or abbreviations.  Another distinction that your point highlights is the difference between readability and comprehension, where faster reading might not correlate with better comprehension.  In fact some natural language research has found that the opposite is true, in what is known as the desirable difficulty effect. 

Once again, I am grateful for your comments and questions which make my current and future research better. Thank you!",NA,"","e-z-reader-model-prediction",112
"97",521698252983238656,"OldCanada#6524","02-Jun-20 02:25 PM","Very nice talk and very interesting work @Naser ...a few things to keep in mind more so than questions.  First, the word length effect also occurs in memory (people can better remember a list of short words than a list of long words) and this is attributable to the speed they can rehearse/maintain the item.  This seems really relevant here too since during programming, I imagine programmers are holding various things in memory and things like length could impact you further.  With regard to the ""the the"" mistake though, it is important to note that this is mostly driven by automaticity rather than word length.  Common words like ""the"" are encountered so frequently as to almost lose meaning.  Your brain just automatically ""knows"" it from years of experience.  Moreover, your brain tries to extract what is intended by a passage more so than the actual words (this is what makes proofreading so difficult, your mind kind of auto-fixes errors).  As another example, if you give people a page to read and ask them to cross out all the letter e's, they will miss the ones in words like ""the"" but will have no problem with things like ""eel"" (same length but lower frequency, lesser experience means less processing fluency).  What I find really neat about your approach here is that it seems like it has the potential to dissociate frequency from length from automatic processing as a function of how the eyes are moving.  Given the clear differences between code and actual written text (and the fact that with more experience, programmers should become more automatic on certain functions and get more frequency of exposure to things), I think you have a very fruitful research program here.  Lots that can be tweaked to answer a lot of really interesting questions",NA,"üíØ (4)","e-z-reader-model-prediction",303
"98",600770279026982912,"peitek#2292","03-Jun-20 02:52 AM","> Hello @peitek, thank you for your kind comments and important questions.
> 
> In regards to your first question, in the presented work we did not split compound tokens, but this idea is definitely interesting and calls for further contemplation in future work.
> 
> To answer your second question, counting token/word frequencies from large corpora of text is intended to estimate how much exposure subjects have had to the language, with subjects who are not native speakers of the language this estimate is more difficult to calculate.  Nonetheless, since the ""over-optimistic"" predictions appeared in natural text and source code I would imagine that this issue can be solved by fine tuning the model parameters to approximate observed data better.
@Naser Thank you for your detailed response. I learned a lot about the e-z model!",NA,"üôÇ (1)","e-z-reader-model-prediction",135
"99",716935205385928832,"Zsofia Pilz#3944","03-Jun-20 08:01 AM","Hello @Naser 
I find the topic and investigation very interesting! Besides, your presentation is very pleasant and you are easy to follow. Regarding the work, I have a question: The E-Z Reader is applied to reading a plain text file and plain code, since the comments have been removed from the code. Couldn't you also leave the comments in the code and thus measure an intermediate level in which text and code are mixed? So that in the end you have 3 sizes that you compare, namely text, code and code + text",NA,"","e-z-reader-model-prediction",94
"100",715584647530020864,"Teresa#9355","03-Jun-20 08:40 AM","Cool! I wonder how the type of Java token is of influence. For instance, there is only a limited set of keywords, the majority of which are probably pretty familiar to programmers. On the other hand, there is an almost endless number of identifiers and a program can contain many that the reader has never seen before. Identifiers can also vary substantially in length. Thus, additional to length and frequency, type of token might be another factor worth looking at.
Furthermore, source code has a very strict structure. Provided it runs without errors, a programmer can be sure that certain elements are present, e.g. if you see ‚Äúpublic static void main ...‚Äù, you don‚Äôt have to read that line thoroughly each time you encounter it, a simple glance might be enough. Thus words in that specific line will get less visual attention than somewhere else in the program.",NA,"","e-z-reader-model-prediction",151
"101",610317742020624384,"Naser#1609","03-Jun-20 09:33 AM","@OldCanada Thank you for your valued comments.  You are correct, in the example with ""the the"" there are a number of factors other than length that cause the eyes to skip.  I think the example with ""eel"" you mentioned makes a lot of sense, and it might be an interesting task to try with source code as a future direction.  I appreciate the valued advice!",NA,"","e-z-reader-model-prediction",66
"102",610317742020624384,"Naser#1609","03-Jun-20 10:16 AM","@Zsofia Pilz Thank you for your kind comment.  I think your idea is very interesting,  studying code comments might inform us of the substrates for the differences between source code and natural language text.  Code comments share the purpose and semantics of source code yet they share the syntax and viewing strategy with natural language text.  This can also be interesting to investigate how programmers leverage their natural language skills in reading source code. Thanks for the interesting idea!",NA,"üëç (1)","e-z-reader-model-prediction",80
"103",610317742020624384,"Naser#1609","03-Jun-20 10:27 AM","@Teresa Thank you for your comment, and thanks for contributing to the data set I am using in this study üôÇ 
I am very interested in investigating the effects of token type, in fact that is something that I am working on at the moment. Your work on element type and frequency from 2014 provides a lot of helpful information for that.  If I might ask, I am curious to know your thoughts on the frequency effect results in the same 2014 study, why do you think the frequency effect was not observed in that study?",NA,"","e-z-reader-model-prediction",97
"104",715839112162050176,"HirotoHarada#0443","31-May-20 06:07 AM","Dear all, here is our paper","https://cdn.discordapp.com/attachments/707137668445634571/716608827121860618/etra20shortpapers-61.pdf","üëç (3)","eye-movement-features",6
"105",715839112162050176,"HirotoHarada#0443","31-May-20 06:12 AM","And this is my presentation
","","üëç (5)","eye-movement-features",5
"106",304446699206868992,"zombieowl#4403","01-Jun-20 09:49 AM","Moderators for this channel are @Marjaana Puurtinen and @zombieowl","","","eye-movement-features",9
"108",521698252983238656,"OldCanada#6524","02-Jun-20 02:37 PM","Hi  @HirotoHarada , this is a very nice, straightforward study.  I will ask one question that I asked of one of the other presenters.  I think there's good rationale for the eye movement metrics you chose as it relates to comprehension, but there may be other very informative ones (I'm assuming the SMI eyetracker extracts a number of metrics automatically).  Run/visit count (I don't know how SMI labels it but the number of times people leave and return to an area),  saccade velocity might be a nice compliment to the length measure, and the speed with which people enter a critical area of interest relative to the start of the trial might also be worthwhile to consider","","","eye-movement-features",120
"109",715839112162050176,"HirotoHarada#0443","03-Jun-20 12:11 AM","Hello @OldCanada 
thank you so much for your very interesting questions. 
We believe that further analysis will be possible by using other indicators and considering them holistically. As you mentioned, it is important to use eye movement features such as saccade velocity and further use of AOI, and it is also important to use parameters such as pupil size to reflect the human attentional state.","","üëç (1)","eye-movement-features",65
"110",707150407104724992,"Marjaana Puurtinen#3084","03-Jun-20 02:29 AM","Hi @HirotoHarada ! I viewed @unaizah s talk first and then yours - you too have a lot in common in terms of what you are after, and put together, your talks present different methods for addressing these reading profile/pattern issues. Interesting! I totally understand your problem with finding a meaningful parameter for regressions (or ""look backs"", as they say it in text reading research; see e.g. Hy√∂n√§, Lorch and Rinch, 2003). In my music-reading studies, too, the freq of regressions is so small (due to time-pressured linear reading; not much time for look backs), that they are very hard to analyze. And you know that each look back is meaningful especially since they are so rare, but you can't get it out in quantitative analyses... I have typically only coded them as ""yes/no"", and not cared about the exact freq. However, in your case, and for future studies, one simple solution might be to rethink the layout of the code. With more meaningful AOIs (longer codes?), there will also be more possibilties for look backs. Another is to attempt to create profiles that take into account 2-3 parameters, with look backs as one of them. I hand it to you for your sample size, which pretty large and enables you to try out different types of analyses in search of the best approach!","","üëç (1)","eye-movement-features",230
"111",600770279026982912,"peitek#2292","03-Jun-20 07:34 AM","Hi @HirotoHarada, thank you for your nice presentation and I'm glad the EMIP dataset is still providing value to new research!

As an outlook to your study, I think it's quite interesting to think about how to structure code. If a programmer follows a control flow of a program, the amount of ""look backs"" highly depends on the order of the methods. In a way, bad source code may force programmers to unnecessary saccades. We go into this topic a little bit with our ""source code linearity index"" (, Sec. 3.1), but I miss research that looks in depth at the relationship between code structure and induced eye movements.","","","eye-movement-features",111
"112",484954298120404992,"JonathanSdlr#1695","03-Jun-20 09:46 AM","Hi, @HirotoHarada . Thank you for your comprehensive presentation. The topic of concern is clear here. I'd like to ask a question about the mean difference outcomes. 

What would you say we can learn from the mean difference in fixation times on the different blocks among those scoring correct or incorrect?  A key finding from this paper is shown in a figure given that the two groups are significantly different in their fixation times across all blocks in at least one of the examples, but I'd like to know if a specific block in Rectangle or Vehicle code stands out.","","","eye-movement-features",101
"113",709663149791641600,"Selina Emhardt#3277","27-May-20 02:17 AM","","https://cdn.discordapp.com/attachments/707137844442955868/715101277079470080/Emhardt_et_al._2020.pdf","üëç (3)","teachers-didactic-guidance",0
"114",709663149791641600,"Selina Emhardt#3277","29-May-20 09:18 AM","And here the video for next week: . I hope you enjoy watching it, and I am looking forward to your comments in this chat!","","üëç (5)","teachers-didactic-guidance",24
"115",600770279026982912,"peitek#2292","01-Jun-20 04:12 AM","Thank you @Selina Emhardt for a great presentation and interesting research. 

I'm going to open the discussion with a question: for your study 2, how do you plan to measure mental effort and learning? You mention that on some preliminary analysis you did not find any significant differences, but I'm wondering if this could depend on what measure you look at?

All other participants: Please don't be shy and ask questions as well!","","","teachers-didactic-guidance",76
"116",709663149791641600,"Selina Emhardt#3277","01-Jun-20 05:03 AM","Thank you for your question, Norman! I am looking forward to the presentations and discussions with the attendants of this workshop. Yes, we did unfortunately not find any significant effects of displayed model behavior on mental effort and learning and we are still discussing possible reasons for that. The used measures could indeed play an important role!
As for the mental effort measure: For this measure, we used a subjective rating scale (based on Paas, 1992). Participants had to rate the subjective amount of mental effort that they invested in following the (natural or didactic) EMME videos on a Likert Scale from 1 until 9. While this scale is quite reliable and frequently used in our field, it is true that there are also more fine-graded measures for mental effort and cognitive load that can, for instance, distinguish between extraneous, intrinsic, germane load. It might worth looking into this in more detail in subsequent studies.","","","teachers-didactic-guidance",156
"117",709663149791641600,"Selina Emhardt#3277","01-Jun-20 05:03 AM","As for the learning measures: Participants performed both a direct recall and a transfer post-test task (for both measures we did not find significant effects). 
Recall task: After watching the (natural or didactic) EMME videos, participants saw inspected screenshots of the codes from the videos and got 2 minutes time for each code to recall and report all errors that they remembered (e.g., writing ‚Äúline 4: missing AND‚Äù). This was a relatively simple task and participants scored overall quite high.
Transfer task: We created new, short codes snippets that contained similar errors as the codes in the EMME videos of the categories misplaced, malformed and missing statements (Fitzgerald et al., 2008; Johnson, Soloway, Cutler, & Draper, 1983). This might have been a more challenging task for the programming beginners, and they showed quite low performances. Maybe we could have found more effects if the codes would have been easier and even more similar to the codes in the videos (even more similar bugs, to ease the transfer).","","","teachers-didactic-guidance",169
"118",715480365191397376,"Standa Popelka#2329","01-Jun-20 06:01 AM","Hi @Selina Emhardt , I would like to ask, whether you didn't try to investigate the strategy of the participants in the sense of order of visited areas... Who started in the  instruction AOI, who looked directly into code etc...","","","teachers-didactic-guidance",40
"119",709663149791641600,"Selina Emhardt#3277","01-Jun-20 06:26 AM","Dear @Standa Popelka , thank you very much for your response to our video and welcome to this chat üôÇ ! There are so many possible ways to analyze eye-tracking data! On the one hand, it is great to work with this incredibly rich type of data. On the other hand, selecting meaningful and relevant measures is also one of our biggest challenges! In our first study, we chose a theory-driven approach. This means that we formulated hypothesis for the selected measures based on known expertise literature (see the first table of my PowerPoint presentation).","","","teachers-didactic-guidance",95
"120",709663149791641600,"Selina Emhardt#3277","01-Jun-20 06:26 AM","I agree that AOI sequences have the potential to provide new insights into experts‚Äô and novices‚Äô debugging strategies. In fact, I just watched the interesting presentation of @unaizah  who worked with AOI sequences/orders to investigate (pseudo)code reading strategies. I would be very interested to hear if you have ideas on how we could ground an AOI-sequence measure in expertise literature. More specifically, what would you expect regarding differences in AOI sequences between (naturally behaving) experts and novices and naturally and didactically behaving experts for our task materials (maybe based on some expertise theories)?","","","teachers-didactic-guidance",96
"121",715480365191397376,"Standa Popelka#2329","01-Jun-20 06:57 AM","@Selina Emhardt - we have recently developed a tool for that kind of analysis called ScanGraph - you can check it here: / or read a paper about it here  or . I suppose you are using SMI software, so you can use directly data from SMI with the converter /. Recently, we wrote a paper where several students of physics were evaluated (also) using this tool - . The one where we are comparing students and their geography teacher (expert) is currently under construction.","","","teachers-didactic-guidance",79
"122",709663149791641600,"Selina Emhardt#3277","01-Jun-20 07:49 AM","Thank you very much, this is interesting! I will look into it!","","","teachers-didactic-guidance",12
"123",600770279026982912,"peitek#2292","01-Jun-20 08:37 AM","Thank you @Selina Emhardt for the detailed response to my question. I think those measures are a good choice.

I agree with @Standa Popelka, there may be potential in the reading order and whether those are different depending on the teaching method. @Teresa et al.'s paper may be a good starting point for this direction as well (","","","teachers-didactic-guidance",58
"124",716539063678795776,"Natalia Chitalkina#5818","01-Jun-20 09:47 AM","Thank you @Selina Emhardt for this interesting talk! I agree with Norman that cognitive load might depend on measure. I assume that in highly competitive programs students might tend to underestimate the level of difficulty of learning something in any kind of official situations that may be used to asssess their abilities (at least I noticed that during my master study where folks tended to assess learning activities as more difficult in face-to-face communications compared to when asked by teachers or program administration). So, it might be interesting to add some physiological parameter such as skin conductance, heart rate etc","","","teachers-didactic-guidance",102
"125",304446699206868992,"zombieowl#4403","01-Jun-20 09:50 AM","Moderators for this channel are @zombieowl and @peitek","","","teachers-didactic-guidance",8
"127",600770279026982912,"peitek#2292","01-Jun-20 10:38 AM","> Thank you @Selina Emhardt for this interesting talk! I agree with Norman that cognitive load might depend on measure. I assume that in highly competitive programs students might tend to underestimate the level of difficulty of learning something in any kind of official situations that may be used to asssess their abilities (at least I noticed that during my master study where folks tended to assess learning activities as more difficult in face-to-face communications compared to when asked by teachers or program administration). So, it might be interesting to add some physiological parameter such as skin conductance, heart rate etc
As a follow to @Natalia Chitalkina 's  suggestion. If you don't feel comfortable with additional physio sensors yet, you could consider capturing pupil dilation. It's commonly used as cognitive load measure and many eye-trackers can capture it. But it does make the setup more complicated to ensure stable light conditions.","","","teachers-didactic-guidance",156
"128",709663149791641600,"Selina Emhardt#3277","01-Jun-20 11:11 AM","Thank you @Natalia Chitalkina for your suggestions! Student‚Äôs self-reported cognitive load might indeed be biased and situation dependent. However, I would also argue that all participants took part in the study in a similar situation and with similar conditions. We therefore hoped that we would still see differences in reported mental effort between the two conditions.","","","teachers-didactic-guidance",58
"129",709663149791641600,"Selina Emhardt#3277","01-Jun-20 11:11 AM","Subsequent studies could indeed benefit from more objective measures. In this context, I like @peitek‚Äôs idea of using pupil dilation. This would be a relatively easy way to add a more objective, physiological measure, too. Our mobile SMI eye tracker captured pupil dilatation in study 1, but I am afraid the light conditions were not controlled enough for now (we recorded the data with a mobile SMI RED Eye Tracker in students‚Äô lecture rooms and experts‚Äô own work offices).","","","teachers-didactic-guidance",80
"130",709663149791641600,"Selina Emhardt#3277","01-Jun-20 11:13 AM","Thank you for the valuable suggestions. As I said earlier, eye tracking offers such rich data that making a selection of the most meaningful/valuable measures is one of the biggest challenges I think!","","","teachers-didactic-guidance",34
"131",304446699206868992,"zombieowl#4403","01-Jun-20 11:14 AM","You are absolutely right @Selina Emhardt and there is no one size fits all.  It changes based on the task.","","","teachers-didactic-guidance",20
"132",716539063678795776,"Natalia Chitalkina#5818","01-Jun-20 12:00 PM","Sure  @Selina Emhardt, it was just my thought, my assumption might be wrong for your sample! Well, in my opinion, physio sensors data are much easier to record and analyze compared to pupil dilation. There are at least three kind of things you need to fix with pupil dilation: lighting in the room, pupil foreshortening error (foreshortening of the pupil image as the eye rotates away from the camera, more in Hayes and Petrov, 2016 paper) and the difference in lighting levels between the screen of the computer and computer keyboard  in case a task includes the use of the keyboard. + the same colors of stimuli and background in all trials/calibrations, and more or less equal visual complexity of all stimuli. All these issues made me not only look for  the ways to fix them, but also check possible alternatives (currently the most popular is the blinking rate, but I tried to pilot saccadic velocity and amplitude, see in my talk).","","","teachers-didactic-guidance",162
"133",715584647530020864,"Teresa#9355","02-Jun-20 08:52 AM","Great to see more research in programming education utilizing eye movements. Combining data on visual behavior with information on mental effort, comprehension etc., like you are doing, yields very rich findings (and is very complex üòâ ).

I‚Äôm looking forward to your future work.","","üëç (1)","teachers-didactic-guidance",44
"134",521698252983238656,"OldCanada#6524","02-Jun-20 02:56 PM","Interesting talk and paper @Selina Emhardt, the EMME technique reminds me of (and I'll be really showing my age here) the old bouncing ball that they used to use on screen in movie theaters during musical pieces...the lyrics would appear on the bottom of the screen and the ball would bounce to each syllable synchronous to the beat of the actual song.  It was essentially how they taught music literacy to mass audiences decades ago and it was quite effective (used on kids shows to teach pronunciation, syllables, etc.).  I can really see the application here.  I wanted to alert you to a couple of techniques/findings that are common in the psychological literature that may help here.  The first is the use of a gaze contingent window.  In your example, you have a dot transposed over the program that represents someone else's eye movements, but it's also possible to do the same thing but only allow people to see the area/items being fixated in a given moment with the rest of the screen appearing blank (you can also do the reverse, make it so people can't see anything they directly fixate...they don't like that as they only get peripheral info).  I've always wondered how this would impact program comprehension.  On the down side, you don't have the global sense of the program, but on the plus side for learning, you are only focusing on the most critical information that the more expert programmer is, independent of any surrounding distraction.  The other finding/technique relates to previewing the program.  There's numerous studies showing that if you give someone a brief preview of something like a visual search display, it drastically changed how they orient to and carry out the search.  I'm thinking the suggestion of EMME is that the dot indicating the eye would be present immediately when the program is presented.  But giving people a brief preview first might facilitate learning and allow them to better block out things that would otherwise be distracting","","üëç (2)","teachers-didactic-guidance",347
"135",709663149791641600,"Selina Emhardt#3277","03-Jun-20 02:19 AM","Dear @OldCanada , thanks for your input and interest in our work! I am happy that you see a useful application in the EMME method :). I know of at least some studies from the field of programming education that worked with a gaze contingent window or restricted focus viewers (e.g., the work of Bednarik & Tukiainen, 2005 and 2007). In these cases, there is a kind of spotlight on the elements than the task performer focuses on while the rest of the task material is blurred. However, the effects of different visualization techniques (moving circle, dot, spotlight etc.) on learning with EMME are, until now, not often investigated. Jarodzka et al. (2012, 2013) showed that for guiding a learner‚Äôs attention (and improving their learning outcomes), it may indeed matter what display design is used. I assume the spotlight visualizations might be most suitable to guide an observer‚Äôs attention through visually complex task material, in which only one specific element is relevant at the time (as the rest of the screen is blurred in this type of visualization). In contrast, a moving dot/circle might be more appropriate if observers also require the information of other areas to make sense of the performer‚Äôs/expert's behavior. I think it is likely that especially programming beginners often require a global sense of the program to follow and understand the expert‚Äôs behavior. However, participants with higher expertise might be able to follow EMME with a spotlight visualization more easily. Investigating effects of different types of eye-movement visualizations on learning with EMME would certainly be a highly interesting avenue for future research!","","","teachers-didactic-guidance",272
"136",709663149791641600,"Selina Emhardt#3277","03-Jun-20 02:25 AM","Concerning your suggestion to provide a code preview before showing the EMME: Our qualitative data shows that participants would indeed prefer such a preview! This might help them to later follow the expert‚Äôs behavior more easily without having to look up unknown information while watching the EMME. In this context, allowing participants to stop the EMME might be also helpful (participants could not stop the video in between during our experiment to keep the exposure lengths equal).","","","teachers-didactic-guidance",78
"137",716972004665851904,"RB#1006","03-Jun-20 03:09 AM","@Selina Emhardt and @OldCanada - interesting discussion about the gaze-contingent paradigm. We indeed did some studies that originally aimed at validating the technique for programming. What happened there was that especially more advanced programmers felt very restricted by not be able to 'peek' into other parts of the source code quickly. I would be very interested in adapting the real-time gaze-contingent paradigm for EMME's - I think we talked about this with @Halszka at some point. Also, the level of attentional cuing (how much the EMME forces somebody looking into specific areas) is something worth exploring.","","üëç (2)","teachers-didactic-guidance",99
"138",709663149791641600,"Selina Emhardt#3277","03-Jun-20 03:55 AM","Hello Roman, it's good to hear from you again! I remember that your study, you showed experts were more impacted/affected by restricting their focus than novices. I am wondering if this extends to understanding model behavior in EMME though. On the one hand, novices might require the additional information from the periphery more than experts in order to make sense of the expert's behavior in the modeling video. On the other hand, experienced programmers might have already develop their own ideosyncratic ways of tackeling programming tasks (e.g., debugging tasks). Consequently, they might be more bothered/affected by the restriction when watching EMME than novices. Adapting the real-time gaze-contingent paradigm for EMME's and also taking into account the learners' prior knowledge level would indeed be very interesting! Are you currently working in this field?","","","teachers-didactic-guidance",140
"139",712199332224303104,"Halszka#6133","03-Jun-20 04:03 AM","great presentation, @Selina Emhardt !","","","teachers-didactic-guidance",4
"140",716972004665851904,"RB#1006","03-Jun-20 04:44 AM","> Hello Roman, it's good to hear from you again! I remember that your study, you showed experts were more impacted/affected by restricting their focus than novices. I am wondering if this extends to understanding model behavior in EMME though. On the one hand, novices might require the additional information from the periphery more than experts in order to make sense of the expert's behavior in the modeling video. On the other hand, experienced programmers might have already develop their own ideosyncratic ways of tackeling programming tasks (e.g., debugging tasks). Consequently, they might be more bothered/affected by the restriction when watching EMME than novices. Adapting the real-time gaze-contingent paradigm for EMME's and also taking into account the learners' prior knowledge level would indeed be very interesting! Are you currently working in this field?
@Selina Emhardt Would be great to work on this in collaboration. We haven't digged deeper lately.","","üëç (2)","teachers-didactic-guidance",158
"141",715584647530020864,"Teresa#9355","03-Jun-20 08:04 AM","Another interesting aspect is to study how the instructions accompanying the EMME facilitate comprehension. It probably makes a difference whether students are only told to watch the EMME or if there are additional prompts like ‚ÄúPay extra attention to the order in which the gaze proceeds over the program‚Äù or ‚ÄúNote how much the gaze alternates between the different areas‚Äù.","","üëç (1)","teachers-didactic-guidance",62
"142",709663149791641600,"Selina Emhardt#3277","03-Jun-20 08:43 AM","Dear @Teresa, thanks for your comment and welcome to our discussion! Maybe a brief not ahead: I read your paper from the ICPC in 2015 in which you also investigated expertise-related differences in code-reading linearity and really liked your work. In fact, we based one of our hypothesis about code-reading linearity on your work :)
Concerning your previous comment: In our studies, we focused, until now, on the effects of different instructions for the model prior to video creation. We have not investigated the effects of instructions for the learners prior to watching the video. I would expect that these kind of instructions have an effect on attention guidance and learning, too. Such instructions could, for instance, be based on previous expertise research such as the findings from study 1. Then, the instructions could highlight different aspects of the EMME that reveal at experts‚Äô (perceptual) strategies and make them more explicitly available to the learners (‚ÄúNote how much the gaze alternates between the code and the output area‚Äù).","","","teachers-didactic-guidance",171
"143",709663149791641600,"Selina Emhardt#3277","03-Jun-20 08:48 AM","Maybe this could possibly increase the effectiveness of EMME, but this is an open research question until now.","","","teachers-didactic-guidance",18
"144",707150407104724992,"Marjaana Puurtinen#3084","03-Jun-20 08:57 AM","Hi @Selina Emhardt , and many thanks for your talk! üôÇ Just a brief note, if I understood your study 2 correctly: I am not too suprised that the two EMME formats you tested have not, in the preliminary analyses at least, brought forth any significant differences in learning outcomes, if you only ran the experiment with one group of students, who might be skilled enough to adapt to any kind of visual instruction (think of the range of YouTube videos many use, and do not find it hard to first learn one thing with one, and then another thing with another). Perhaps different EMMEs would work for students from different age groups (eg kids learning programming, adults learning it at a later age...) and this has something to do with experience with the domain and visual instruction tools. Just a thought! üôÇ","","","teachers-didactic-guidance",142
"145",709663149791641600,"Selina Emhardt#3277","03-Jun-20 09:03 AM","Dear @Marjaana Puurtinen. Thank you, for your valuable comment! Our group of participants was indeed rather homogeneous and this could (partially) explain our non-significant findings (we are currently still working on the discussion section of study 2).","","","teachers-didactic-guidance",38
"146",709665121315848192,"Florian Hauser#1774","27-May-20 02:42 AM","Dear all, 
this is my paper.","https://cdn.discordapp.com/attachments/707137908456554526/715107580816654376/Hauser_et_al._2020.pdf","üëç (3)","code-reviews-in-cplusplus",6
"147",709665121315848192,"Florian Hauser#1774","31-May-20 07:19 PM","Dear all, here is the link to my presentation: 
I am looking forward for all of your questions, feedback and comments. 
If you need any further information, please feel free to chat wit me or to email me.

I would also like to thank the EMIP-committee for organizing the workshop in a virtual way. 2020 is a very strange year for all of us and I am happy that we can at least do a whole conference in this way.
Please  enjoy the workshop and stay healthy! üëç","","üëç (5)","code-reviews-in-cplusplus",89
"148",709665121315848192,"Florian Hauser#1774","31-May-20 07:21 PM","Here is also a link to my Google Drive, including a mp4-file of my presentation: ","","üëç (2)","code-reviews-in-cplusplus",16
"149",304446699206868992,"zombieowl#4403","01-Jun-20 09:50 AM","Moderators for this channel are @Marjaana Puurtinen and @peitek","","","code-reviews-in-cplusplus",9
"151",600770279026982912,"peitek#2292","01-Jun-20 10:36 AM","@Florian Hauser Thank you for your interesting talk!

I have a couple of clarification questions:
- You mentioned that the snippets did not have any syntax highlighting. Can you elaborate what was your reasoning?
- Could you share one or two snippets as examples? I'm curious how complex the snippets are and what kind of errors you've added.

Your preliminary analysis looks very reasonable. In my eye-tracking data I have seen similar patterns (and I'm guilty of being one of the researchers who use Java.. üòÜ). Have you found anything that seems different for C++ than for all the Java studies?

On slide 11, you discuss the correlation between eye movements and performance. I'm wondering of both are mainly driven by the programming expertise rather than a direct cause-effect relationship. With increased skill, you change they way you read code, but also will perform better.

I'm looking forward to the finished analysis and in particular the comparison to the C study.

Everyone else, don't be shy and ask questions as well!","","","code-reviews-in-cplusplus",178
"152",709665121315848192,"Florian Hauser#1774","02-Jun-20 04:06 AM","Hello @peitek,

thank you very much for your very interesting questions. 
To the first part of your question: We have decided not to use syntax highligting for a number of reasons. On the one hand, we wanted to create a situation that is relatively unknown to both experts and novices. On the other hand, we also wanted to maintain a certain comparability with our studies in C, where we also used stimuli without highlighting. 
Maybe also interesting: We are currently conducting a study on the role of highlighting methods in which we compare syntax and semantic highlighting in debugging tasks (this time we are also using Java ... üòâ).
 I will uplaod some stimuli to Google Drive later and provide a link to them.","","","code-reviews-in-cplusplus",123
"153",716539063678795776,"Natalia Chitalkina#5818","02-Jun-20 09:42 AM","Halo, @Florian Hauser ! I'm glad to see the progress of your research since our last chat at Prof Gruber's lab meeting! Could you clarify what kind of qualitative interpretation was used in the analysis? I just thought that it might be interesting to mix eye tracking with content analysis of verbal comments here. What kind of parameters are you interested to analyze in TOIs to study review phases? Are you planning to describe the differences between phases in experts with this analysis, because it seems like there is only one phase in beginners due to your current description? I'm looking forward to see the comparison to your C study and new highlighting study findings","","","code-reviews-in-cplusplus",117
"154",521698252983238656,"OldCanada#6524","02-Jun-20 02:59 PM","I wish I had a question to ask on this but I think this is on the right track and I look forward to seeing additional results @Florian Hauser .  There's a lot of interesting things to look at relating to expertise and I think applying this to other programming languages/platforms would also afford a determination of whether their are general skills shared by experts across programming languages or if expertise is unique to the langue you program in","","","code-reviews-in-cplusplus",80
"155",600770279026982912,"peitek#2292","03-Jun-20 02:45 AM","> Hello @peitek,
> 
> thank you very much for your very interesting questions. 
> To the first part of your question: We have decided not to use syntax highligting for a number of reasons. On the one hand, we wanted to create a situation that is relatively unknown to both experts and novices. On the other hand, we also wanted to maintain a certain comparability with our studies in C, where we also used stimuli without highlighting. 
> Maybe also interesting: We are currently conducting a study on the role of highlighting methods in which we compare syntax and semantic highlighting in debugging tasks (this time we are also using Java ... üòâ).
>  I will uplaod some stimuli to Google Drive later and provide a link to them.
@Florian Hauser Nice! We are also debating for every study if we should use syntax highlighting or not. A study investigating this would be quite helpful!","","","code-reviews-in-cplusplus",150
"156",600770279026982912,"peitek#2292","03-Jun-20 02:47 AM","> I wish I had a question to ask on this but I think this is on the right track and I look forward to seeing additional results @Florian Hauser .  There's a lot of interesting things to look at relating to expertise and I think applying this to other programming languages/platforms would also afford a determination of whether their are general skills shared by experts across programming languages or if expertise is unique to the langue you program in
The studies I have seen in this direction indicate that it's a given that programmers can easily switch languages. The transfer of knowledge from one language to another is an extensive process (e.g., ","","","code-reviews-in-cplusplus",116
"157",709665121315848192,"Florian Hauser#1774","03-Jun-20 08:33 AM","Hello @Natalia Chitalkina , nice to hear from you! Thank you very much for your questions. 
Yeah, things have moved on since the last meeting. 

The retrospective interviews were conducted in such a way that we showed the participants their own eye movementes and had them comment on them. Simmultaneously we did an audio record of the whole process. During the evaluation of the interviews I consulted one of our programmers and watched the gaze record and audio records with him. We were mainly interested if certain strategies were mentioned and whether these corresponded with the eye movements. In some cases the test persons described their procedure very precisely, which made the evaluation quite easy. The experts often mentioned the approach to start at main and get a first overview of the code. In a next step I would like to transcribe the interviews and do a detailed content analysis with interraters. I am currently still thinking about how to create a suitable category system.

The TOI based evaluation should indeed clarify differences in the approaches of experts and novices. I always asked the subjects how ofthen they had to read the code until they understood it. I will use the mentioned number of passes to differentiate between two phases (probably ""scan"" and ""error detection""). Here I am particularly interested in how the scan paths change in these phases. Based on the gaze record, I expect a relatively clear change for the experts. For the novices it does unfortunately not look so obvious  ... üò¨","","","code-reviews-in-cplusplus",253
"158",707150407104724992,"Marjaana Puurtinen#3084","03-Jun-20 08:33 AM","Hello @Florian Hauser ! Many thanks for your talk! I was wondering that since years of general programming as well as professional programming correlated with task performanc, did you check if you could (or even should) divide the participants into more than two groups? Especially if you are getting more participants, I was thinking if you could get more detailed idea of the learning trajectories from novices to intermediates and on to expert programmers. I know the intermediate group is often a problem (it being a very heterogeneous group) in the analysis, but are you exactly sure that you only have the two groups? Would a three-way grouping help to pinpoint in which ways intermediates are (still) more like novices or already acting like experts? Thinking of pedagogical implications and targeting training approriately, this would be interesting!","","","code-reviews-in-cplusplus",137
"159",712199332224303104,"Halszka#6133","03-Jun-20 08:35 AM","Hi @Florian Hauser, I very much like your thoughts about experts building a mental model! That's something that is really difficult to study, but could potentially be really valuable for our understanding of expertise in this field. Moreover, I wanted to point out the same issue as @Marjaana Puurtinen Seeing the high expertise of your example expert, looking into intra-group differences might be truly interesting!","","","code-reviews-in-cplusplus",67
"160",709665121315848192,"Florian Hauser#1774","03-Jun-20 09:13 AM","Hello @Marjaana Puurtinen! 

Thank you very much for your feedback. I am also thinking about dividing the sample into three groups. It would be very interesting to see the in what way the intermediates differ from novices and experts.
I am getting ahead of myself here. üòÖ Here are some findings from the extended sample:

In January we had the chance to expand our sample and to test a larger group of experts. These were mainly programmers who are specialized in C++. They are further developing the language and set standards for companies. In some cases, these participants have 15 to 25 years of professional experience in programming (and doing code reviews almost on a daily basis). In terms of error detection, they obviously scored very well. But so did many PhD students in the sample, too. The main difference between the PhD students and the experts was that the latter were even more efficient and much faster in conducting the reviews. With increasing professional experience there seem to be clear differences again. At the moment, I can only speculate, but I think that over the years the experts have simply seen a lot more ""typical bugs"" and have thus developed a better sense of which parts of the code are particularly prone to errors.

So I actually had the plan to rate the PhD students as intermediates and recruit more of the experts (now that a contact has been established). More data collections were planned for March /April, but then Corona unfortunately got in the way ... ü§£  Well, let's see what the future brings ...","","","code-reviews-in-cplusplus",266
"161",707150407104724992,"Marjaana Puurtinen#3084","03-Jun-20 09:20 AM","> @Florian Hauser Okay, sounds good! üôÇ Best of luck with your work!
> 
> Thank you very much for your feedback. I am also thinking about dividing the sample into three groups. It would be very interesting to see the in what way the intermediates differ from novices and experts.
> I am getting ahead of myself here. üòÖ Here are some findings from the extended sample:
> 
> In January we had the chance to expand our sample and to test a larger group of experts. These were mainly programmers who are specialized in C++. They are further developing the language and set standards for companies. In some cases, these participants have 15 to 25 years of professional experience in programming (and doing code reviews almost on a daily basis). In terms of error detection, they obviously scored very well. But so did many PhD students in the sample, too. The main difference between the PhD students and the experts was that the latter were even more efficient and much faster in conducting the reviews. With increasing professional experience there seem to be clear differences again. At the moment, I can only speculate, but I think that over the years the experts have simply seen a lot more ""typical bugs"" and have thus developed a better sense of which parts of the code are particularly prone to errors.
> 
> So I actually had the plan to rate the PhD students as intermediates and recruit more of the experts (now that a contact has been established). More data collections were planned for March /April, but then Corona unfortunately got in the way ... ü§£  Well, let's see what the future brings ...
@Florian Hauser","","üëç (1)","code-reviews-in-cplusplus",278
